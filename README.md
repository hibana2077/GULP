# GULP: Gated Unimodal Loudness Pulse

## Abstract

Activation functions critically shape the optimization landscape of deep networks. We propose GULP, a smooth, self-gated activation that augments the Swish/SiLU family with a localized “pulse” term. Formally, GULP multiplies a Swish gate with a unimodal Gaussian bump centered near the positive region, yielding a mild non-monotonic uplift around the activation threshold while retaining Swish-like tails. This design aims to enhance gradient flow near moderate positive pre-activations with negligible overhead. GULP is drop-in compatible with MLP, CNN, and Transformer feed-forward layers, and can also serve as the gate in GLU-style blocks. We provide the mathematical formulation, derivatives, and practical hyperparameter settings to facilitate adoption.