# GULP: A Smooth Self‑Gated Activation with a Localized Pulse

## Abstract

Activation functions critically shape the optimization landscape of deep networks. We propose GULP, a smooth, self‑gated activation that augments the Swish/SiLU family with a localized pulse (bump) term. Formally, GULP multiplies a Swish gate with a unimodal Gaussian bump centered near the positive region, yielding a mild non‑monotonic uplift around the activation threshold while retaining Swish‑like tails. This design enhances gradient flow around moderate positive pre‑activations with negligible overhead. GULP is drop‑in compatible with MLP, CNN, and Transformer feed‑forward layers, and can also serve as the gate in GLU‑style blocks. We provide the mathematical formulation, derivatives, and a comprehensive empirical evaluation across vision, language, audio, graphs, and tabular tasks.